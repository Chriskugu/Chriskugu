{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpLmqHihOxTxGBKpBlZEAS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chriskugu/Chriskugu/blob/main/final_projet_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real -world Dataset:"
      ],
      "metadata": {
        "id": "TIL7bqvhmTIc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCjk-IkOfdg8"
      },
      "outputs": [],
      "source": [
        "https://covid-api.com/api/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://covid-api.com/api/regions?per_page=20&order=iso&sort=asc"
      ],
      "metadata": {
        "id": "_bsIaZhRg56E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adequacy check\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch data\n",
        "url = \"https://covid-api.com/api/regions?per_page=20&order=iso&sort=asc\"  # Max records\n",
        "response = requests.get(url)\n",
        "data = response.json()['data']\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Check adequacy\n",
        "print(f\"Total regions: {len(df)}\")\n",
        "print(\"Variables:\\n\", df.columns)\n",
        "print(\"Missing values:\\n\", df.isnull().sum())\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX2eoomrnTqW",
        "outputId": "20b7d41a-a807-4ab4-80dc-9bd143b97e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total regions: 20\n",
            "Variables:\n",
            " Index(['iso', 'name'], dtype='object')\n",
            "Missing values:\n",
            " iso     0\n",
            "name    0\n",
            "dtype: int64\n",
            "['iso', 'name']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Integrity\n",
        "\n",
        "Source: COVID-19 API aggregates data from WHO, Johns Hopkins, and government reports.\n",
        "\n",
        "Potential Biases:\n",
        "\n",
        "  Underreporting in low-income regions.\n",
        "\n",
        "  Delays in last_update (e.g., some countries report weekly).\n",
        "\n",
        "Variable Clarity:\n",
        "\n",
        "  iso: Standardized country codes (reliable).\n",
        "\n",
        "active: May exclude asymptomatic cases (potential underestimation)."
      ],
      "metadata": {
        "id": "YPolYLNsus1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning and Preparation"
      ],
      "metadata": {
        "id": "HEcjOv702g6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Fetch the data from the API\n",
        "url = \"https://covid-api.com/api/regions?per_page=20&order=iso&sort=asc\"\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data['data'])\n",
        "\n",
        "# Initial inspection\n",
        "print(\"Initial data shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Basic cleaning\n",
        "# Convert date fields to datetime\n",
        "if 'last_update' in df.columns:\n",
        "    df['last_update'] = pd.to_datetime(df['last_update'])\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(subset=['iso', 'name'], inplace=True)  # Essential identifier fields\n",
        "\n",
        "# Type conversion for numeric fields\n",
        "numeric_cols = ['confirmed', 'deaths', 'recovered', 'confirmed_diff', 'deaths_diff', 'recovered_diff']\n",
        "for col in numeric_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Calculate derived metrics\n",
        "if all(col in df.columns for col in ['deaths', 'confirmed']):\n",
        "    df['fatality_rate'] = df['deaths'] / df['confirmed']\n",
        "\n",
        "if all(col in df.columns for col in ['recovered', 'confirmed']):\n",
        "    df['recovery_rate'] = df['recovered'] / df['confirmed']\n",
        "\n",
        "# Final cleaning\n",
        "# Remove completely empty columns\n",
        "df.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "# Validate cleaned data\n",
        "print(\"\\nCleaned data shape:\", df.shape)\n",
        "print(\"\\nCleaned data sample:\")\n",
        "print(df.head())\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv('cleaned_covid_regions_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjeq-LFwoD2z",
        "outputId": "7a340da7-220f-4bb8-cc7c-ef36878bc678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data shape: (20, 2)\n",
            "\n",
            "First few rows:\n",
            "   iso         name\n",
            "0  ABW        Aruba\n",
            "1  AFG  Afghanistan\n",
            "2  AGO       Angola\n",
            "3  ALB      Albania\n",
            "4  AND      Andorra\n",
            "\n",
            "Data types:\n",
            "iso     object\n",
            "name    object\n",
            "dtype: object\n",
            "\n",
            "Missing values:\n",
            "iso     0\n",
            "name    0\n",
            "dtype: int64\n",
            "\n",
            "Cleaned data shape: (20, 2)\n",
            "\n",
            "Cleaned data sample:\n",
            "   iso         name\n",
            "0  ABW        Aruba\n",
            "1  AFG  Afghanistan\n",
            "2  AGO       Angola\n",
            "3  ALB      Albania\n",
            "4  AND      Andorra\n",
            "\n",
            "Missing values after cleaning:\n",
            "iso     0\n",
            "name    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Values & Outliers"
      ],
      "metadata": {
        "id": "7evlWZvi3P0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1:Missing Values"
      ],
      "metadata": {
        "id": "9I82EUhq5bT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Check missing values percentage\n",
        "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
        "print(\"Missing values percentage:\\n\", missing_percent)\n",
        "\n",
        "### Strategy 1: Drop columns with high missingness\n",
        "# Drop columns with >70% missing values\n",
        "threshold = 70\n",
        "cols_to_drop = missing_percent[missing_percent > threshold].index.tolist()\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "print(f\"Dropped columns with >{threshold}% missing: {cols_to_drop}\")\n",
        "\n",
        "### Strategy 2: Impute remaining missing values\n",
        "# For numerical columns\n",
        "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "for col in num_cols:\n",
        "    if df[col].isnull().any():\n",
        "        # Use median for numerical columns (less sensitive to outliers)\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "        print(f\"Imputed {col} with median: {df[col].median()}\")\n",
        "\n",
        "# For categorical columns\n",
        "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in cat_cols:\n",
        "    if df[col].isnull().any():\n",
        "        # Use mode for categorical columns\n",
        "        mode_val = df[col].mode()[0]\n",
        "        df[col] = df[col].fillna(mode_val)\n",
        "        print(f\"Imputed {col} with mode: {mode_val}\")\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(\"\\nMissing values after treatment:\\n\", df.isnull().sum())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c_Wm6UovEo",
        "outputId": "39b20032-6dde-4bb1-d32a-6fb2f1339b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values percentage:\n",
            " iso     0.0\n",
            "name    0.0\n",
            "dtype: float64\n",
            "Dropped columns with >70% missing: []\n",
            "\n",
            "Missing values after treatment:\n",
            " iso     0\n",
            "name    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Outliers"
      ],
      "metadata": {
        "id": "xl3uK0Om5XOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 2: Handling Outliers (Fixed Version)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "### Step 2.1: Detect outliers - Only for columns that exist\n",
        "# First identify which COVID metrics actually exist in our DataFrame\n",
        "existing_metrics = [col for col in ['confirmed', 'deaths', 'recovered',\n",
        "                   'confirmed_diff', 'deaths_diff', 'recovered_diff',\n",
        "                   'fatality_rate'] if col in df.columns]\n",
        "\n",
        "if not existing_metrics:\n",
        "    print(\"Warning: No COVID metrics columns found for outlier detection\")\n",
        "else:\n",
        "    # Visualize outliers only for existing columns\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    df[existing_metrics].boxplot()\n",
        "    plt.title('Boxplot of COVID-19 Metrics (Before Outlier Treatment)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    ### Step 2.2: Treat outliers\n",
        "    def cap_outliers(series):\n",
        "        Q1 = series.quantile(0.25)\n",
        "        Q3 = series.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        return series.clip(lower_bound, upper_bound)\n",
        "\n",
        "    # Apply to existing numerical COVID metrics\n",
        "    for metric in existing_metrics:\n",
        "        original_median = df[metric].median()\n",
        "        df[metric] = cap_outliers(df[metric])\n",
        "        print(f\"\\nOutlier treatment for {metric}:\")\n",
        "        print(f\"  - Median before: {original_median}\")\n",
        "        print(f\"  - Median after: {df[metric].median()}\")\n",
        "\n",
        "        # Calculate bounds for reporting\n",
        "        Q1 = df[metric].quantile(0.25)\n",
        "        Q3 = df[metric].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        print(f\"  - Lower bound: {lower_bound}\")\n",
        "        print(f\"  - Upper bound: {upper_bound}\")\n",
        "        print(f\"  - Values capped: {sum((df[metric] <= lower_bound) | (df[metric] >= upper_bound))}\")\n",
        "\n",
        "    # Visualize after treatment\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    df[existing_metrics].boxplot()\n",
        "    plt.title('Boxplot of COVID-19 Metrics (After Outlier Treatment)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    ### Step 2.3: Special handling for rates and counts\n",
        "    # For fatality_rate (ensure between 0 and 1)\n",
        "    if 'fatality_rate' in df.columns:\n",
        "        df['fatality_rate'] = df['fatality_rate'].clip(0, 1)\n",
        "        print(\"\\nClipped fatality_rate to [0, 1] range\")\n",
        "\n",
        "    # For counts (ensure non-negative)\n",
        "    count_cols = ['confirmed', 'deaths', 'recovered']\n",
        "    for col in count_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].clip(lower=0)\n",
        "            print(f\"Ensured {col} values are non-negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLUomApX5Pvn",
        "outputId": "b4f37bd8-c926-4a27-8bb7-8e2d8f2007c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No COVID metrics columns found for outlier detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Validation"
      ],
      "metadata": {
        "id": "UILWIHxX9LZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_covid_data(df):\n",
        "    \"\"\"Robust validation function for COVID-19 data\"\"\"\n",
        "\n",
        "    print(\"\\n=== DATA VALIDATION REPORT ===\")\n",
        "\n",
        "    # 1. Basic Structure\n",
        "    print(\"\\n[1] Data Structure:\")\n",
        "    print(f\"Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
        "    print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "    # 2. COVID Metrics Check\n",
        "    covid_metrics = {\n",
        "        'cases': ['confirmed', 'cases', 'total_cases'],\n",
        "        'deaths': ['deaths', 'fatalities', 'total_deaths'],\n",
        "        'recovered': ['recovered', 'total_recovered']\n",
        "    }\n",
        "\n",
        "    found_metrics = {}\n",
        "    for metric, aliases in covid_metrics.items():\n",
        "        for alias in aliases:\n",
        "            if alias in df.columns:\n",
        "                found_metrics[metric] = alias\n",
        "                break\n",
        "\n",
        "    print(\"\\n[2] COVID Metrics Found:\")\n",
        "    for metric, col_name in found_metrics.items():\n",
        "        print(f\"- {metric}: {col_name} (dtype: {df[col_name].dtype})\")\n",
        "\n",
        "    if not found_metrics:\n",
        "        print(\"No standard COVID metrics found in dataset\")\n",
        "\n",
        "    # 3. Data Quality Metrics\n",
        "    print(\"\\n[3] Data Quality:\")\n",
        "    print(\"Missing Values:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    if found_metrics:\n",
        "        print(\"\\nBasic Statistics:\")\n",
        "        print(df[list(found_metrics.values())].describe())\n",
        "\n",
        "    # 4. Save Validated Data\n",
        "    output_file = 'validated_covid_data.csv'\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"\\n[4] Validation Complete - Data saved to {output_file}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Run the validation\n",
        "df_validated = validate_covid_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v29h3i_07173",
        "outputId": "c3ed0104-5e5b-41cc-968f-4f7cf27934bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DATA VALIDATION REPORT ===\n",
            "\n",
            "[1] Data Structure:\n",
            "Rows: 219, Columns: 2\n",
            "Columns: ['iso', 'name']\n",
            "\n",
            "[2] COVID Metrics Found:\n",
            "No standard COVID metrics found in dataset\n",
            "\n",
            "[3] Data Quality:\n",
            "Missing Values:\n",
            "iso     0\n",
            "name    0\n",
            "dtype: int64\n",
            "\n",
            "[4] Validation Complete - Data saved to validated_covid_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recoding & Encoding Variables"
      ],
      "metadata": {
        "id": "2MH8csiP7Ntj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recoding Variables"
      ],
      "metadata": {
        "id": "vXIqbkbz7jwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First examine categorical variables\n",
        "cat_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(\"Categorical variables:\", cat_vars)\n",
        "\n",
        "### Example: Recode region names to standardized format\n",
        "if 'name' in df.columns:\n",
        "    df['name'] = df['name'].str.title().str.strip()\n",
        "    print(\"\\nStandardized region names\")\n",
        "\n",
        "### Example: Create binary indicators for specific regions\n",
        "if 'name' in df.columns:\n",
        "    df['is_high_risk'] = df['name'].isin(['Wuhan', 'Lombardy', 'Madrid']).astype(int)\n",
        "    print(\"\\nCreated high-risk region indicator\")\n",
        "\n",
        "### Example: Recode date into pandemic waves\n",
        "if 'last_update' in df.columns:\n",
        "    df['last_update'] = pd.to_datetime(df['last_update'])\n",
        "\n",
        "    # Define pandemic wave periods (adjust dates as needed)\n",
        "    wave_periods = [\n",
        "        ('Initial Wave', '2019-12-01', '2020-06-30'),\n",
        "        ('Delta Wave', '2021-04-01', '2021-10-31'),\n",
        "        ('Omicron Wave', '2021-11-01', '2022-03-31'),\n",
        "        ('Recent Period', '2022-04-01', '2023-12-31')\n",
        "    ]\n",
        "\n",
        "    df['pandemic_wave'] = 'Other'\n",
        "    for wave, start, end in wave_periods:\n",
        "        mask = (df['last_update'] >= start) & (df['last_update'] <= end)\n",
        "        df.loc[mask, 'pandemic_wave'] = wave\n",
        "\n",
        "    print(\"\\nRecoded dates into pandemic waves\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pua7FG6D7BC6",
        "outputId": "8cfd9f9e-ff31-405e-e2a2-c100a39719b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical variables: ['iso', 'name']\n",
            "\n",
            "Standardized region names\n",
            "\n",
            "Created high-risk region indicator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding Variables"
      ],
      "metadata": {
        "id": "3Z5Rb_9S9wt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "### Option 1: Label Encoding (for ordinal categories)\n",
        "if 'pandemic_wave' in df.columns:\n",
        "    wave_order = ['Initial Wave', 'Delta Wave', 'Omicron Wave', 'Recent Period', 'Other']\n",
        "    le = LabelEncoder()\n",
        "    le.fit(wave_order)\n",
        "    df['pandemic_wave_encoded'] = le.transform(df['pandemic_wave'])\n",
        "    print(\"\\nLabel encoded pandemic waves:\")\n",
        "    print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "\n",
        "### Option 2: One-Hot Encoding (for nominal categories)\n",
        "if 'name' in df.columns:\n",
        "    # Get top n regions to avoid too many dummy variables\n",
        "    top_regions = df['name'].value_counts().nlargest(10).index.tolist()\n",
        "    df['region_group'] = df['name'].where(df['name'].isin(top_regions), 'Other')\n",
        "\n",
        "    # Perform one-hot encoding\n",
        "    dummies = pd.get_dummies(df['region_group'], prefix='region')\n",
        "    df = pd.concat([df, dummies], axis=1)\n",
        "    print(\"\\nOne-hot encoded top regions:\")\n",
        "    print(dummies.columns.tolist())\n",
        "\n",
        "### Option 3: Target Encoding (for high-cardinality categories)\n",
        "if 'name' in df.columns and 'fatality_rate' in df.columns:\n",
        "    # Calculate mean fatality rate by region\n",
        "    region_means = df.groupby('name')['fatality_rate'].mean().to_dict()\n",
        "    df['region_fatality_encoded'] = df['name'].map(region_means)\n",
        "    print(\"\\nTarget encoded regions by fatality rate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5HXWWdK941G",
        "outputId": "67f6c381-6849-45d4-b5a5-0c1aa581d02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One-hot encoded top regions:\n",
            "['region_China', 'region_Japan', 'region_Korea, South', 'region_Malaysia', 'region_Other', 'region_Philippines', 'region_Singapore', 'region_Taipei And Environs', 'region_Thailand', 'region_Us', 'region_Vietnam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data Transformation  for Skewed COVID-19 Variables"
      ],
      "metadata": {
        "id": "lTljpL5u-vsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Identify Skewed Variables"
      ],
      "metadata": {
        "id": "e3yiWttFAH10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Select numeric COVID metrics (excluding binary/categorical)\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "covid_metrics = [col for col in numeric_cols if col not in ['is_high_risk', 'region_encoded']]\n",
        "\n",
        "print(\"Skewness before transformation:\")\n",
        "skew_before = df[covid_metrics].skew()\n",
        "print(skew_before)\n",
        "\n",
        "# Visualize distributions\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(covid_metrics, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(f'{col} (Skew: {skew_before[col]:.2f})')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "v_dyHfP6APBV",
        "outputId": "536944e3-ffe2-442f-bf3b-8fd6fb3e1dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skewness before transformation:\n",
            "Series([], dtype: float64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Apply Appropriate Transformations"
      ],
      "metadata": {
        "id": "V0f1kPRdAU1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logarithmic Transformation (for right-skewed data)\n",
        "\n",
        "right_skewed = skew_before[skew_before > 1].index.tolist()\n",
        "\n",
        "for col in right_skewed:\n",
        "    # Add 1 to handle zeros (log(0) is undefined)\n",
        "    df[f'log_{col}'] = np.log1p(df[col])\n",
        "\n",
        "print(\"\\nApplied log1p transformation to:\", right_skewed)\n",
        "\n",
        "# Square Root Transformation (moderate right skew)\n",
        "\n",
        "moderate_skew = skew_before[(skew_before > 0.5) & (skew_before <= 1)].index.tolist()\n",
        "\n",
        "for col in moderate_skew:\n",
        "    df[f'sqrt_{col}'] = np.sqrt(df[col])\n",
        "\n",
        "print(\"Applied sqrt transformation to:\", moderate_skew)\n",
        "\n",
        "# Box-Cox Transformation (for positive values with varying skewness)\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "for col in covid_metrics:\n",
        "    if df[col].min() > 0:  # Box-Cox requires positive values\n",
        "        transformed, _ = stats.boxcox(df[col])\n",
        "        df[f'boxcox_{col}'] = transformed\n",
        "        print(f\"Applied Box-Cox to {col}\")\n",
        "\n",
        "\n",
        "# Yeo-Johnson Transformation (handles zero/negative values)\n",
        "\n",
        "for col in covid_metrics:\n",
        "    transformed, _ = stats.yeojohnson(df[col])\n",
        "    df[f'yeojohnson_{col}'] = transformed\n",
        "    print(f\"Applied Yeo-Johnson to {col}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHtblqP1-1Jc",
        "outputId": "1c0f4b89-27dd-4d61-f8b0-860d6d95fa36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applied log1p transformation to: []\n",
            "Applied sqrt transformation to: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Validate Transformation Results"
      ],
      "metadata": {
        "id": "1_dlYhuwBYmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate skewness after transformation\n",
        "transformed_cols = [f'log_{col}' for col in right_skewed] + \\\n",
        "                  [f'sqrt_{col}' for col in moderate_skew] + \\\n",
        "                  [f'boxcox_{col}' for col in covid_metrics if f'boxcox_{col}' in df] + \\\n",
        "                  [f'yeojohnson_{col}' for col in covid_metrics]\n",
        "\n",
        "print(\"\\nSkewness after transformation:\")\n",
        "print(df[transformed_cols].skew())\n",
        "\n",
        "# Visualize transformed distributions\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(transformed_cols, 1):\n",
        "    plt.subplot(4, 4, i)\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "dqs2TEUxBg4e",
        "outputId": "0a3d3d69-a3d2-47f8-a842-25a4e6e5ad0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Skewness after transformation:\n",
            "Series([], dtype: float64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization and Standardization for COVID-19 Data Analysis"
      ],
      "metadata": {
        "id": "BlHg-0QCB6nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First identify which variables need scaling\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# Remove columns that shouldn't be scaled:\n",
        "# - Binary variables (0/1)\n",
        "# - Variables already on comparable scales (e.g., percentages)\n",
        "# - Identifier variables (region codes, etc.)\n",
        "to_exclude = [col for col in numeric_cols if\n",
        "              df[col].nunique() == 2 or  # Binary\n",
        "              col.endswith('_encoded') or  # Already encoded\n",
        "              col in ['iso', 'region_code']]  # Identifiers\n",
        "\n",
        "features_to_scale = [col for col in numeric_cols if col not in to_exclude]\n",
        "\n",
        "print(\"Variables to scale:\", features_to_scale)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT9GJIuaDIgQ",
        "outputId": "88d1a399-befa-4cd0-c217-77a634cf3458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variables to scale: ['is_high_risk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardization (Z-score Normalization)"
      ],
      "metadata": {
        "id": "_S3TBAkTGYDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\" Algorithms assuming Gaussian distributions (PCA, SVM, linear regression)\n",
        "\n",
        "When outliers are properly handled\n",
        "\n",
        "Features with different units but comparable ranges\"\"\""
      ],
      "metadata": {
        "id": "16JVTF0eHFar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = df.copy()\n",
        "df_scaled[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
        "\n",
        "# Verify standardization\n",
        "print(\"\\nAfter standardization (mean ~0, std ~1):\")\n",
        "print(df_scaled[features_to_scale].describe().loc[['mean', 'std']])\n",
        "\n",
        "# Save standardized data\n",
        "df_scaled.to_csv('covid_data_standardized.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0SB2JCKGffw",
        "outputId": "1209015b-7207-458d-eb4d-b1f3503133fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After standardization (mean ~0, std ~1):\n",
            "      is_high_risk\n",
            "mean           0.0\n",
            "std            0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Min-Max Normalization"
      ],
      "metadata": {
        "id": "fxKswe_-HWkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\" Neural networks\n",
        "\n",
        "Algorithms requiring [0,1] range (e.g., KNN)\n",
        "\n",
        "Preserving zero entries in sparse data\"\"\""
      ],
      "metadata": {
        "id": "M7E5ozgwHh4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "mmscaler = MinMaxScaler()\n",
        "df_normalized = df.copy()\n",
        "df_normalized[features_to_scale] = mmscaler.fit_transform(df[features_to_scale])\n",
        "\n",
        "# Verify normalization\n",
        "print(\"\\nAfter normalization (range [0,1]):\")\n",
        "print(df_normalized[features_to_scale].describe().loc[['min', 'max']])\n",
        "\n",
        "# Save normalized data\n",
        "df_normalized.to_csv('covid_data_normalized.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyMia4XtHooO",
        "outputId": "ada68f31-af3b-4268-e8e1-79ec2cf8294b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After normalization (range [0,1]):\n",
            "     is_high_risk\n",
            "min           0.0\n",
            "max           0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robust Scaling (for COVID-19 Data with Outliers)"
      ],
      "metadata": {
        "id": "4aIt5ofEIMBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "rscaler = RobustScaler()\n",
        "df_robust = df.copy()\n",
        "df_robust[features_to_scale] = rscaler.fit_transform(df[features_to_scale])\n",
        "\n",
        "# Verify robust scaling\n",
        "print(\"\\nAfter robust scaling (median=0, IQR=1):\")\n",
        "print(pd.DataFrame({\n",
        "    'median': df_robust[features_to_scale].median(),\n",
        "    'IQR': df_robust[features_to_scale].quantile(0.75) - df_robust[features_to_scale].quantile(0.25)\n",
        "}))\n",
        "\n",
        "# Save robust scaled data\n",
        "df_robust.to_csv('covid_data_robust_scaled.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afLX7KX0ISjr",
        "outputId": "b5349652-f467-4c89-e467-e60599417745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After robust scaling (median=0, IQR=1):\n",
            "              median  IQR\n",
            "is_high_risk     0.0  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COVID-Specific Scaling Considerations"
      ],
      "metadata": {
        "id": "CKCK5nmDIqel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'population' in df.columns:\n",
        "    # Create per-capita features before scaling\n",
        "    df['cases_per_100k'] = (df['confirmed'] / df['population']) * 100000\n",
        "    df['deaths_per_100k'] = (df['deaths'] / df['population']) * 100000\n",
        "    features_to_scale.extend(['cases_per_100k', 'deaths_per_100k'])"
      ],
      "metadata": {
        "id": "G5cJEmxHIx0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time-Based Normalization"
      ],
      "metadata": {
        "id": "-LFnfC9lKPDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'date' in df.columns:\n",
        "    # Normalize within each time period\n",
        "    df['month'] = df['date'].dt.month\n",
        "    for col in ['confirmed', 'deaths']:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_norm_by_month'] = df.groupby('month')[col].transform(\n",
        "                lambda x: (x - x.min()) / (x.max() - x.min()))"
      ],
      "metadata": {
        "id": "WTSBNtYzKWKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing the Right Method"
      ],
      "metadata": {
        "id": "izRzT7RMKg9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaling_recommendations = {\n",
        "    'kmeans': 'StandardScaler',\n",
        "    'logistic_regression': 'StandardScaler',\n",
        "    'svm': 'StandardScaler',\n",
        "    'neural_network': 'MinMaxScaler',\n",
        "    'knn': 'MinMaxScaler',\n",
        "    'decision_tree': 'None needed',\n",
        "    'random_forest': 'None needed'\n",
        "}\n",
        "\n",
        "print(\"\\nScaling recommendations by algorithm:\")\n",
        "for algo, scaler in scaling_recommendations.items():\n",
        "    print(f\"{algo:>20}: {scaler}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0mfKoW6KoUD",
        "outputId": "c9e4b891-8d2b-4332-f745-2900ff27a39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scaling recommendations by algorithm:\n",
            "              kmeans: StandardScaler\n",
            " logistic_regression: StandardScaler\n",
            "                 svm: StandardScaler\n",
            "      neural_network: MinMaxScaler\n",
            "                 knn: MinMaxScaler\n",
            "       decision_tree: None needed\n",
            "       random_forest: None needed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Implementation"
      ],
      "metadata": {
        "id": "NKxFDDnDKvYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example pipeline\n",
        "if 'outcome' in df.columns:  # Replace with your target variable\n",
        "    X = df[features_to_scale]\n",
        "    y = df['outcome']\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    # Create pipeline with scaling + classifier\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),  # Can swap with other scalers\n",
        "        ('classifier', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    print(f\"\\nModel accuracy: {pipeline.score(X_test, y_test):.2f}\")"
      ],
      "metadata": {
        "id": "il3yJBH6K0c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\" COVID-Specific Factors:\n",
        "\n",
        "Case counts often follow power-law distributions (consider log transform first)\n",
        "\n",
        "Regional comparisons benefit from population-normalized features\n",
        "\n",
        "Time-dependent normalization accounts for pandemic waves\n",
        "\n",
        "When to Scale:\n",
        "\n",
        "Always: Distance-based algorithms (KNN, SVM, K-means)\n",
        "\n",
        "Usually: Neural networks, linear models\n",
        "\n",
        "Rarely: Tree-based methods\n",
        "\n",
        "Validation:\n",
        "\n",
        "Always check descriptive statistics after scaling\n",
        "\n",
        "Verify no information leakage (fit scalers on training data only)\n",
        "\n",
        "Document which features were scaled and which method was used\n",
        "\n",
        "Special Cases:\n",
        "\n",
        "For sparse data (many zeros), MinMax may be better than Standard\n",
        "\n",
        "For datasets with extreme outliers, Robust scaling is preferred\n",
        "\n",
        "For compositional data (percentages), consider isometric log-ratio transforms\n",
        "\n",
        "Remember that the choice between normalization and standardization depends on your specific machine learning task and the nature of your COVID-19 data features. Always validate the impact of scaling on your model performance.\"\"\""
      ],
      "metadata": {
        "id": "pmgog8SLK8SE"
      }
    }
  ]
}